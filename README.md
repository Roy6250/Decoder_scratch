# Decoder from Scratch

This repository contains an implementation of a **Transformer Decoder from scratch** using PyTorch. It's a minimal and educational re-creation of the core architecture that powers modern large language models (LLMs), like GPT.

---

##  Features

- Manual implementation of the Transformer Decoder block
- Self-attention mechanism
- Positional encoding
- Masking for auto-regressive generation
- Clean and minimal PyTorch code
- Great for learning and experimentation

---

## Architecture

This implementation includes:

- **Token Embedding Layer**
- **Positional Encoding**
- **Multi-head Self Attention**
- **Feedforward Neural Network**
- **Layer Normalization & Residual Connections**
- **Causal Masking (for left-to-right prediction)**

---

# Training Loss for 2000 iterations


